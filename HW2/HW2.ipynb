{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKvA2ee9KUG_",
        "colab_type": "code",
        "outputId": "87cc90e4-955c-447d-ae90-ae7ea1dd77a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        }
      },
      "source": [
        "#!pip3 install 'torch==1.4.0'\n",
        "#!pip3 install 'torchvision==0.5.0'\n",
        "#!pip3 install 'Pillow-SIMD'\n",
        "#!pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 19kB/s \n",
            "\u001b[31mERROR: torchvision 0.6.0+cu101 has requirement torch==1.5.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Found existing installation: torch 1.5.0+cu101\n",
            "    Uninstalling torch-1.5.0+cu101:\n",
            "      Successfully uninstalled torch-1.5.0+cu101\n",
            "Successfully installed torch-1.4.0\n",
            "Collecting torchvision==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.18.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (7.0.0)\n",
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.4.0)\n",
            "Installing collected packages: torchvision\n",
            "  Found existing installation: torchvision 0.6.0+cu101\n",
            "    Uninstalling torchvision-0.6.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.6.0+cu101\n",
            "Successfully installed torchvision-0.5.0\n",
            "Collecting Pillow-SIMD\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/6a/30d21c886293cca3755b8e55de34137a5068b77eba1c0644d3632080516b/Pillow-SIMD-7.0.0.post3.tar.gz (630kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 8.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Pillow-SIMD\n",
            "  Building wheel for Pillow-SIMD (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pillow-SIMD: filename=Pillow_SIMD-7.0.0.post3-cp36-cp36m-linux_x86_64.whl size=1110357 sha256=9f6ba629bde41b8c63c31253d892766e41c0bd4dcf83bb75d16416fbb4292259\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/ac/4f/4cdf8febba528e5f1b09fc58d5181e1c12ed1e8655dcd583b8\n",
            "Successfully built Pillow-SIMD\n",
            "Installing collected packages: Pillow-SIMD\n",
            "Successfully installed Pillow-SIMD-7.0.0.post3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn6cPW3oLFOv",
        "colab_type": "code",
        "outputId": "c3ed5209-99c0-4e7b-cceb-54ac60a506cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Homework2-Caltech101'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 9280 (delta 10), reused 14 (delta 6), pack-reused 9262\u001b[K\n",
            "Receiving objects: 100% (9280/9280), 129.49 MiB | 34.07 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "Checking out files: 100% (9149/9149), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_zYWjtXKZlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUaMh6zLMr5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone github repository with data\n",
        "if not os.path.isdir('./Caltech101'):\n",
        "\t!git clone https://github.com/rm-wu/Homework2-Caltech101.git\n",
        "\t!mv 'Homework2-Caltech101' 'Caltech101'\n",
        "\n",
        "from Caltech101.caltech_dataset import Caltech\n",
        "from torch.utils.data.dataset import random_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ_XNn3mKg_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 102 # 101 + 1: There is an extra Background class that should be removed \n",
        "\n",
        "BATCH_SIZE = 256    # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "\t\t\t\t\t# the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 1e-3           # The initial Learning Rate\n",
        "MOMENTUM = 0.9      # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5 # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 30     # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 20      # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1         # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-RqFgM2KiXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                   # Remember this when applying different transformations, otherwise you get an error\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizes tensor with mean and standard deviation\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3ezDxjsKlrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeM3EatrKmuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = './Caltech101'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb2IweQAKusn",
        "colab_type": "code",
        "outputId": "e44bfb20-de68-4df1-9c81-bb586cf0583d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#from Caltech101.caltech_dataset import Caltech#\n",
        "#from torch.utils.data.dataset import random_split\n",
        "\n",
        "# Prepare Pytorch train/test Datasets\n",
        "train_dataset_ = Caltech(DATA_DIR, split='train',  transform=train_transform)\n",
        "test_dataset= Caltech(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "#lengths = [int(len(train_dataset)*0.5), int(len(train_dataset)*0.5)]\n",
        "#train_dataset, val_dataset = random_split(train_dataset, lengths)\n",
        "\n",
        "train_idx, val_idx = train_test_split(np.arange(0, len(train_dataset_)), train_size=0.5,\n",
        "\t\t\t\t\t\t\t\t\tshuffle=True, random_state=42, stratify=train_dataset_.y)\n",
        "train_dataset = Subset(train_dataset_, train_idx)\n",
        "val_dataset = Subset(train_dataset_, val_idx)\n",
        "\n",
        "\n",
        "#train_indexes = # split the indices for your train split\n",
        "#val_indexes = # split the indices for your val split\n",
        "\n",
        "#train_dataset = Subset(train_dataset, train_indexes)\n",
        "#val_dataset = Subset(train_dataset, val_indexes)\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Train Dataset: {}'.format(len(train_dataset)))\n",
        "print('Valid Dataset: {}'.format(len(val_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Dataset: 2892\n",
            "Valid Dataset: 2892\n",
            "Test Dataset: 2893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w0pcLoYKw8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaNv-56fKx44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = alexnet() # Loading AlexNet model\n",
        "\n",
        "# AlexNet has 1000 output neurons, corresponding to the 1000 ImageNet's classes\n",
        "# We need 101 outputs for Caltech-101\n",
        "net.classifier[6] = nn.Linear(4096, NUM_CLASSES) # nn.Linear in pytorch is a fully connected layer\n",
        "                                                 # The convolutional layer is nn.Conv2d\n",
        "\n",
        "# We just changed the last layer of AlexNet with a new fully connected layer with 101 outputs\n",
        "# It is strongly suggested to study torchvision.models.alexnet source code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40UbdHTpY241",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = 1e-2           # The initial Learning Rate\n",
        "MOMENTUM = 0.9      # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 1e-4 # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 45     # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 30      # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1         # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6-QS4mPK2sG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "\n",
        "# Choose parameters to optimize\n",
        "# To access a different set of parameters, you have to access submodules of AlexNet\n",
        "# (nn.Module objects, like AlexNet, implement the Composite Pattern)\n",
        "# e.g.: parameters of the fully connected layers: net.classifier.parameters()\n",
        "# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) \n",
        "parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet\n",
        "\n",
        "# Define optimizer\n",
        "# An optimizer updates the weights based on loss\n",
        "# We use SGD with momentum\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Define scheduler\n",
        "# A scheduler dynamically changes learning rate\n",
        "# The most common schedule is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t57QTH-VK4XV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fe-DbB4Lrp5",
        "colab_type": "code",
        "outputId": "05ee4cec-3b86-48d3-ad63-29e31bdf75b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import copy\n",
        "#import time\n",
        "\n",
        "dataloader = {'train': train_dataloader, 'val': val_dataloader}\n",
        "dataset = {'train': train_dataset, 'val': val_dataset}\n",
        "path_1 = \"./Caltech101/weights.pth\" #TODO: for all paths\n",
        "\n",
        "\n",
        "val_info = [[], []]\n",
        "train_info = [[], []]\n",
        "\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=30, save_weights=False, ):\n",
        "    #since = time.time()\n",
        "    cudnn.benchmark\n",
        "    model.to(DEVICE)\n",
        "    best_model_w = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} LR = {scheduler.get_last_lr()}\")\n",
        "        print('=' * 50)\n",
        "        \n",
        "        for phase in ['train', 'val']:\n",
        "            #print(phase)\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "                \n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            \n",
        "            for images, labels in dataloader[phase]:\n",
        "                images = images.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                with torch.set_grad_enabled(phase=='train'):\n",
        "                    outputs = model(images)\n",
        "                    _, preds = torch.max(outputs.data, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        \n",
        "                running_loss += loss.item() * images.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                \n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "            epoch_loss = running_loss / len(dataset[phase])\n",
        "            epoch_acc = running_corrects.double() / len(dataset[phase])\n",
        "\n",
        "            if phase == 'train':\n",
        "                train_info[0].append(epoch_loss)\n",
        "                train_info[1].append(epoch_acc)\n",
        "            else:\n",
        "                val_info[0].append(epoch_loss)\n",
        "                val_info[1].append(epoch_acc)\n",
        "            \n",
        "            print(f\"{phase}\\tLoss: {epoch_loss:.4f}\\tAcc: {epoch_acc:.4f}\")\n",
        "            \n",
        "            if phase == 'val'and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_w = copy.deepcopy(model.state_dict())\n",
        "                if save_weights:\n",
        "                    torch.save(net.state_dict(), path_1)\n",
        "\n",
        "        print()\n",
        "    \n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "    \n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_w)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_scratch = train_model(net, criterion, optimizer, scheduler, num_epochs=NUM_EPOCHS)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 4.4988\tAcc: 0.0526\n",
            "val\tLoss: 4.6106\tAcc: 0.0920\n",
            "\n",
            "Epoch 2/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 4.4779\tAcc: 0.0906\n",
            "val\tLoss: 4.5826\tAcc: 0.0920\n",
            "\n",
            "Epoch 3/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 4.4419\tAcc: 0.0906\n",
            "val\tLoss: 4.5274\tAcc: 0.0920\n",
            "\n",
            "Epoch 4/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 4.2945\tAcc: 0.0899\n",
            "val\tLoss: 4.2486\tAcc: 0.0920\n",
            "\n",
            "Epoch 5/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 4.1391\tAcc: 0.0947\n",
            "val\tLoss: 4.2159\tAcc: 0.0920\n",
            "\n",
            "Epoch 6/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 4.0968\tAcc: 0.0864\n",
            "val\tLoss: 4.2095\tAcc: 0.0923\n",
            "\n",
            "Epoch 7/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 4.0751\tAcc: 0.0882\n",
            "val\tLoss: 4.1299\tAcc: 0.1058\n",
            "\n",
            "Epoch 8/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 4.0174\tAcc: 0.1252\n",
            "val\tLoss: 4.0822\tAcc: 0.1470\n",
            "\n",
            "Epoch 9/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 3.9523\tAcc: 0.1442\n",
            "val\tLoss: 4.0410\tAcc: 0.1971\n",
            "\n",
            "Epoch 10/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 3.8916\tAcc: 0.1805\n",
            "val\tLoss: 3.9226\tAcc: 0.1943\n",
            "\n",
            "Epoch 11/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 3.7981\tAcc: 0.1909\n",
            "val\tLoss: 3.8384\tAcc: 0.2123\n",
            "\n",
            "Epoch 12/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 3.7159\tAcc: 0.2106\n",
            "val\tLoss: 3.7216\tAcc: 0.2254\n",
            "\n",
            "Epoch 13/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 3.6337\tAcc: 0.2196\n",
            "val\tLoss: 3.6746\tAcc: 0.2293\n",
            "\n",
            "Epoch 14/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 3.5636\tAcc: 0.2210\n",
            "val\tLoss: 3.5995\tAcc: 0.2355\n",
            "\n",
            "Epoch 15/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 3.5125\tAcc: 0.2306\n",
            "val\tLoss: 3.5526\tAcc: 0.2427\n",
            "\n",
            "Epoch 16/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 3.4828\tAcc: 0.2286\n",
            "val\tLoss: 3.5470\tAcc: 0.2510\n",
            "\n",
            "Epoch 17/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 3.4314\tAcc: 0.2420\n",
            "val\tLoss: 3.5315\tAcc: 0.2611\n",
            "\n",
            "Epoch 18/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 3.3320\tAcc: 0.2521\n",
            "val\tLoss: 3.4017\tAcc: 0.2680\n",
            "\n",
            "Epoch 19/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 3.2344\tAcc: 0.2566\n",
            "val\tLoss: 3.3835\tAcc: 0.2742\n",
            "\n",
            "Epoch 20/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 3.1491\tAcc: 0.2784\n",
            "val\tLoss: 3.3311\tAcc: 0.2746\n",
            "\n",
            "Epoch 21/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 3.0677\tAcc: 0.2908\n",
            "val\tLoss: 3.2166\tAcc: 0.3043\n",
            "\n",
            "Epoch 22/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 2.9503\tAcc: 0.3053\n",
            "val\tLoss: 3.0902\tAcc: 0.3192\n",
            "\n",
            "Epoch 23/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 2.8682\tAcc: 0.3167\n",
            "val\tLoss: 3.0563\tAcc: 0.3323\n",
            "\n",
            "Epoch 24/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 2.7531\tAcc: 0.3458\n",
            "val\tLoss: 3.0760\tAcc: 0.3309\n",
            "\n",
            "Epoch 25/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 2.6769\tAcc: 0.3489\n",
            "val\tLoss: 2.9429\tAcc: 0.3658\n",
            "\n",
            "Epoch 26/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 2.6200\tAcc: 0.3575\n",
            "val\tLoss: 2.9179\tAcc: 0.3613\n",
            "\n",
            "Epoch 27/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 2.5168\tAcc: 0.3786\n",
            "val\tLoss: 2.7944\tAcc: 0.3873\n",
            "\n",
            "Epoch 28/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 2.3654\tAcc: 0.4122\n",
            "val\tLoss: 2.8322\tAcc: 0.3880\n",
            "\n",
            "Epoch 29/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 2.3244\tAcc: 0.4115\n",
            "val\tLoss: 2.7765\tAcc: 0.3869\n",
            "\n",
            "Epoch 30/45 LR = [0.01]\n",
            "==================================================\n",
            "train\tLoss: 2.1960\tAcc: 0.4402\n",
            "val\tLoss: 2.6530\tAcc: 0.4066\n",
            "\n",
            "Epoch 31/45 LR = [0.001]\n",
            "==================================================\n",
            "train\tLoss: 2.0197\tAcc: 0.4751\n",
            "val\tLoss: 2.6662\tAcc: 0.4257\n",
            "\n",
            "Epoch 32/45 LR = [0.001]\n",
            "==================================================\n",
            "train\tLoss: 1.8512\tAcc: 0.5159\n",
            "val\tLoss: 2.6264\tAcc: 0.4343\n",
            "\n",
            "Epoch 33/45 LR = [0.001]\n",
            "==================================================\n",
            "train\tLoss: 1.7677\tAcc: 0.5370\n",
            "val\tLoss: 2.6282\tAcc: 0.4419\n",
            "\n",
            "Epoch 34/45 LR = [0.001]\n",
            "==================================================\n",
            "train\tLoss: 1.7269\tAcc: 0.5391\n",
            "val\tLoss: 2.6361\tAcc: 0.4402\n",
            "\n",
            "Epoch 35/45 LR = [0.001]\n",
            "==================================================\n",
            "train\tLoss: 1.6850\tAcc: 0.5512\n",
            "val\tLoss: 2.6103\tAcc: 0.4419\n",
            "\n",
            "Epoch 36/45 LR = [0.001]\n",
            "==================================================\n",
            "train\tLoss: 1.6695\tAcc: 0.5564\n",
            "val\tLoss: 2.6490\tAcc: 0.4474\n",
            "\n",
            "Epoch 37/45 LR = [0.001]\n",
            "==================================================\n",
            "train\tLoss: 1.6622\tAcc: 0.5588\n",
            "val\tLoss: 2.6362\tAcc: 0.4523\n",
            "\n",
            "Epoch 38/45 LR = [0.001]\n",
            "==================================================\n",
            "train\tLoss: 1.6237\tAcc: 0.5577\n",
            "val\tLoss: 2.6433\tAcc: 0.4443\n",
            "\n",
            "Epoch 39/45 LR = [0.001]\n",
            "==================================================\n",
            "train\tLoss: 1.5863\tAcc: 0.5743\n",
            "val\tLoss: 2.6196\tAcc: 0.4537\n",
            "\n",
            "Epoch 40/45 LR = [0.001]\n",
            "==================================================\n",
            "train\tLoss: 1.5255\tAcc: 0.5799\n",
            "val\tLoss: 2.6223\tAcc: 0.4519\n",
            "\n",
            "Epoch 41/45 LR = [0.001]\n",
            "==================================================\n",
            "train\tLoss: 1.5164\tAcc: 0.5799\n",
            "val\tLoss: 2.6263\tAcc: 0.4544\n",
            "\n",
            "Epoch 42/45 LR = [0.001]\n",
            "==================================================\n",
            "train\tLoss: 1.4848\tAcc: 0.5913\n",
            "val\tLoss: 2.6389\tAcc: 0.4530\n",
            "\n",
            "Epoch 43/45 LR = [0.001]\n",
            "==================================================\n",
            "train\tLoss: 1.4537\tAcc: 0.5985\n",
            "val\tLoss: 2.6690\tAcc: 0.4568\n",
            "\n",
            "Epoch 44/45 LR = [0.001]\n",
            "==================================================\n",
            "train\tLoss: 1.4598\tAcc: 0.5954\n",
            "val\tLoss: 2.6317\tAcc: 0.4658\n",
            "\n",
            "Epoch 45/45 LR = [0.001]\n",
            "==================================================\n",
            "train\tLoss: 1.4140\tAcc: 0.5989\n",
            "val\tLoss: 2.6248\tAcc: 0.4599\n",
            "\n",
            "Best val Acc: 0.465768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zahyovVMbeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(net.state_dict(), path_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0ZOf1q7MifQ",
        "colab_type": "code",
        "outputId": "cf2eadb3-b4ab-4058-9307-4d660b1427d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "len(train_dataset.dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5784"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qGYsBM_NG6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}